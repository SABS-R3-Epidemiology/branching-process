##### The inverse problem: estimating $R\_t$

With our branching process model one can predict future case numbers based on three pieces of information: the initial number of cases $I\_0$, the reproduction number over time $\{R\_0, R\_1, R\_2, ... \}$, and the serial interval distribution $\{w\_0, w\_1, w\_2, ... \}$. These are the parameters of the model, i.e. the numbers that serve as the model's inputs, as opposed to the model's output, i.e. the predicted case numbers $\{I\_1, I\_2, I\_3, ... \}$. If we know the parameter values, then the model can predict the future case numbers. For example, virologists might have very precise estimates of the serial interval, public health data might give a good estimate of $I\_0$, and epidemiologists might be able to provide reasonable estimates for the reproduction number over time. Predicting future case numbers when the model parameters are known is known as a forward problem (since we are looking forward in time into the future). We have already described how the Branchpro model solves the forward model.

Suppose now that we know the values of $I\_0$ and the serial interval distribution $\{w\_0, w\_1, w\_2, ... \}$ but that we do not know what the reproduction number is. This is a problem if we are hoping to predict future case numbers (i.e. the forward problem). In this scenario, our first task is to come up with an estimate of the current reproduction number. One of the most common ways of doing this is to infer it from past case numbers. For example, given the case numbers for the last 10 days, we can infer the value of the reproduction number at present by assuming that those case numbers were simulated by the Branchpro model where the reproduction number was constant over those 10 days. This is called an inverse problem: the goal is to use case numbers to infer the value of a parameter, the opposite of the forward problem. Our web app can solve this inverse problem.

##### Bayesian inference
This section outlines some of the mathematical details of how the current reproduction number, $R\_t$, is inferred from past case numbers. We will not discuss the mathematics in depth but rather will give the main idea for Bayes' rule and Bayesian inference.

The method is in the category of Bayesian inference, a broad inference paradigm whereby our *prior* knowledge of a probability distribution is combined with new *evidence* to yield the *posterior* probability distribution. In our case the evidence that we use to update and improve our prior knowledge of $R\_t$ is the past daily case numbers over a number of days. Our prior "knowledge" of $R\_t$ is, in reality, an assumption we make about its probability distribution, namely that $R\_t$ follows a gamma distribution. When inferring R_t, we assume (as is commonly done by epidemiologists) that the reproduction number is constant over a sliding window of width tau days, which has the effect of reducing variability in the inference results.

The foundational tool of Bayesian inference is Bayes' rule (a.k.a. Bayes' theorem), which we outline presently. Suppose we want to know something about the world and call this something $\theta$. In our case, $\theta$ is simply the reproduction number on day $t$, i.e. $R\_t$. To apply Bayes' rule, we must represent our current knowledge about the nature of $\theta$ as a *prior* probability distribution, denoted by $p(\theta)$. In our case, we are assuming that $p(\theta)$ corresponds to a gamma distribution. Now suppose that new evidence comes to light that can inform our knowledge of $\theta$ and call this evidence $y$. In our case, $y$ is the list of case numbers $\{I\_{t-\tau}, I\_{t-\tau+1}, ..., I_t \}$. Bayes' rule allows us to update our knowledge of $\theta$ by incorporating the evidence $y$, yielding a *posterior* probability distribution, denoted by $\mathbb{P}(A \vert B)$ (verbally: "the probability distribution of $\theta$ given that the evidence $y$ has been observed", or simply "the probability of $\theta$ given $y$"). Without further ado, Bayes' rule states the following:
$$
p(\theta \vert y) = \frac{p(y \vert \theta) \times p(\theta)}{p(y)}
$$
A corollary of Bayes' rule that is easier to use in our context is the following:
$$
p(\theta \vert y) \propto p(y \vert \theta) \times p(\theta)
$$
(The $\propto$ symbol means "is proportional to".) We have already assumed that we know the prior distribution, $p(\theta)$, i.e. we have assumed that $p(R\_t)$ corresponds to a gamma distribution. Notice also that we know how to compute $p(y \vert \theta)$ with our forward model: recalling what $\theta$ and $y$ are in our case, we have $$p(y \vert \theta) = p(I\_{t-\tau}, I\_{t-\tau+1}, ..., I_t \vert R_t),$$ i.e. the probability of observing said case numbers if the reproduction number on every one of those days was constant and equal to $R\_t$. This is precisely what our forward model can calculate.

The remaining mathematical details of the above calculations of the posterior distribution of $R\_t$ given the evidence $I\_{t-\tau}, I\_{t-\tau+1}, ..., I\_t$ are beyond the scope of this outline and so we omit them. It suffices to say that we can show that the posterior distribution is a gamma distribution with shape parameter $a + \sum\_{k=t-\tau}^{t} I\_k$ and scale parameter $\left( \sum\_{k=t-\tau}^{t} \sum\_{s=1}^{k} I\_{k-s} w\_s + \frac{1}{b} \right)^{-1}$, where $a$ and $b$ are the shape and scale parameters of the prior gamma distribution, respectively. This allows us to calculate an expected value for $R\_t$ as well as "error bars" which indicate that we are, for example, 95% confident that $R\_t$ lies between the lower and upper error bar limits, given that we observed the evidence $I\_{t-\tau}, I\_{t-\tau+1}, ..., I\_t$. Thus, we have achieved what we set out to do, namely to infer the value of $R\_t$ from past case numbers and to quantify the uncertainty involved in our inferred estimate.
